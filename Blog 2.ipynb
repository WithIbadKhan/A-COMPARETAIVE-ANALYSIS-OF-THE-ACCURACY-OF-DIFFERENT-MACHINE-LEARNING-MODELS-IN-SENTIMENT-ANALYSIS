{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1GqZu6zmCy2vMNMZqO78DDuHvwv_9vJcp","timestamp":1665935197000},{"file_id":"1IzPbxNFQmmSYk9s14L4YjBfUgACn9mW2","timestamp":1593978024316},{"file_id":"1G7NKeneJNyRtcRxLVbbF9jYtRyuTOa-R","timestamp":1592749700622},{"file_id":"https://github.com/satyajitghana/TSAI-DeepVision-EVA4.0/blob/master/Utils/Colab_25GBRAM_GPU.ipynb","timestamp":1592043804148}],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"4cbgwZWWfWpp"},"source":["# New Section"]},{"cell_type":"code","metadata":{"id":"tAb77yZ9fzMG"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pafL7Li0jyXW"},"source":["#Pyhton\n","#pandas\n","#numpy\n","#seaborn\n","#matplotlib\n","#skilearn"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import CountVectorizer\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n"],"metadata":{"id":"CyB7O_lUTYOW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#df1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/FYP Project',encoding='utf-8')\n","df1 = pd.read_csv('GrammarandProductReviews.csv',encoding='utf-8')\n"],"metadata":{"id":"vJmbvXS6TbUV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1"],"metadata":{"id":"EiSo1shnTbnD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1.shape"],"metadata":{"id":"nGXXSTyNTb4V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1.head()"],"metadata":{"id":"BHYD41AYTb8l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1.columns"],"metadata":{"id":"odvdQn1LTcBi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#selecting only two columns and assign to selected_df\n","selected_df = df1[['reviews.text','reviews.rating']].copy()\n","#selected_df = selected_df[55000:].copy()\n","selected_df"],"metadata":{"id":"kzpIy71VTcFZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["selected_df.info()"],"metadata":{"id":"AOxcHpH4TcIW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["selected_df = selected_df.rename(columns = {'reviews.text':'Reviews_text','reviews.rating':'Sentiment'})\n","selected_df"],"metadata":{"id":"0xS4sXElTcLP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# #give reviews with Score > 3 a positive rating and reviews with a score < 3 a negative rating\n","# def partition(x):\n","#      if x < 3:\n","#         return 'Negative'\n","#      else:\n","#         return 'Positive'\n","\n","# actualScore = selected_df['Reviews_text_stars']\n","# pos_neg = actualScore.map(partition)\n","# selected_df['Reviews_text_stars'] = pos_neg\n","\n","# selected_df.head()"],"metadata":{"id":"9IbAT7n_TcTR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# selected_df.Reviews_text_stars = selected_df.Reviews_text_stars.astype(int)"],"metadata":{"id":"JM8UnieXUG9C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["selected_df['Sentiment'].value_counts().sort_values(ascending = False)\n","#selected_df.Sentiment = selected_df.Sentiment.astype(int)\n"],"metadata":{"id":"SESKXKO2UHAY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #Plot the distribution graph\n","# sns.set_theme(style=\"darkgrid\")\n","# x = sns.countplot(selected_df['Sentiment'])\n","# from google.colab import files\n","# plt.savefig(\"x.png\")\n","# files.download(\"x.png\")"],"metadata":{"id":"LG9ODfeEUHFR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Training dataset\n","\"\"\"\" Now we consider those reviews whose star is 4 and 5 which will be consider positive review and whose\n","star is less than 5 they will be consider negative reviews\"\"\"\n","selected_df = selected_df.loc[selected_df['Sentiment'] != 3]\n","selected_df.drop_duplicates()"],"metadata":{"id":"qFeqXVWRUHJZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train dataset 5 and 4 stars are will be consider positive and below 3 are consider negative\n","sns.set_theme(style=\"darkgrid\")\n","\n","x = sns.countplot(selected_df['Sentiment'])"],"metadata":{"id":"cJBFW6EkUHM3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#count positive and negative of Train data set\n","selected_df['Sentiment'].value_counts()"],"metadata":{"id":"loPqC5O7UHP1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##Visualize the given reviews with Score > 3 a Positive review and reviews with a score < 3 a Negative review\n","sns.set_theme(style=\"darkgrid\")\n","x = sns.countplot(selected_df['Sentiment'])"],"metadata":{"id":"B-2QBMqKUHS0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#give reviews with Score > 3 a positive rating and reviews with a score < 3 a negative rating\n","def partition(x):\n","     if x < 4:\n","        return 'Negative'\n","     else:\n","        return 'Positive'\n","\n","actualScore = selected_df['Sentiment']\n","pos_neg = actualScore.map(partition)\n","selected_df['Sentiment'] = pos_neg\n","\n","selected_df.head()"],"metadata":{"id":"oGTdApxpUHVi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train dataset visualization\n","sns.set_theme(style=\"darkgrid\")\n","x = sns.countplot(selected_df['Sentiment'])"],"metadata":{"id":"h2tzsiGwUHZf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from sklearn.model_selection import train_test_split\n","# TRAIN_SIZE = 0.8\n","# df_train, df_test = train_test_split(selected_df, test_size=1-TRAIN_SIZE, random_state=42)\n","# #We have stored data in X and target in y. We have aslo printed the shape of the data.\n","# selected_df['reviews'], test_df['reviews'], selected_df['reviews_stars'], test_df['reviews_stars'] = train_test_split(selected_df, test_df, test_size=0.33, random_state=42)"],"metadata":{"id":"3OcJKyBUUHc6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(\"TRAIN size:\", len(selected_df))\n","# print(\"TEST size:\", len(test_df))"],"metadata":{"id":"C1CBKDDyUHg0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %%time\n","# documents = [_text.split() for _text in df_train.reviews]"],"metadata":{"id":"Wikx9Wl2UHjr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip install gensim\n","# !pip install keras\n","# W2V_SIZE = 200\n","# W2V_WINDOW = 5\n","# W2V_EPOCH = 32\n","# W2V_MIN_COUNT = 10\n","# import gensim\n","# w2v_model = gensim.models.word2vec.Word2Vec(documents)\n","# w2v_model.most_similar(\"love\")"],"metadata":{"id":"N0AqAO_cUHmk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train reviews vectors\n","train_reviews = selected_df['Reviews_text'].astype('U').str.lower()\n","train_reviews"],"metadata":{"id":"6JX9DMlnUHp4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Data exploratory"],"metadata":{"id":"0IxrHVR_UHst"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #Train dataset vectors\n","# count_vect = CountVectorizer(analyzer='word', ngram_range=(2,2))\n","# cleaned = []\n","# count_vect.fit(train_reviews)\n","# x = count_vect.transform(train_reviews)\n","# cleaned = x.toarray()\n","# train_v = pd.DataFrame(cleaned, columns=count_vect.get_feature_names_out())\n","# train_v\n","# #Train dataset vectors\n","# count_vect = CountVectorizer(analyzer='word', ngram_range=(2,2))\n","# cleaned = []\n","# count_vect.fit(test_reviews)\n","# x = count_vect.transform(test_reviews)\n","# cleaned = x.toarray()\n","# test_v = pd.DataFrame(cleaned, columns=count_vect.get_feature_names_out())\n","# test_v"],"metadata":{"id":"xuj3w11iUHv2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","#Let's see the number of words averaged for positive and negative reviews.\n","selected_df['Sentiment'].fillna(\"\")\n","\n","# Get mean of positive and negative reviews\n","avg_pos_reviews = selected_df[selected_df['Sentiment']=='Positive'].Reviews_text.astype(str).apply(lambda x: len(x.split(\" \"))).mean()\n","avg_neg_reviews = selected_df[selected_df['Sentiment']=='Negative'].Reviews_text.astype(str).apply(lambda x: len(x.split(\" \"))).mean()\n","\n","plt.figure(figsize=(7,2))\n","plt.barh(['Positive', 'Negative'], [avg_pos_reviews, avg_neg_reviews], height=0.5)\n","plt.xticks(np.arange(0, 10, 25))\n","plt.xlabel('Average Number of words')\n","plt.ylabel('Sentiment')\n","plt.show()"],"metadata":{"id":"rJ9U40NjUHzA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train the model the Unigram features\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","X = selected_df['Reviews_text'].values.astype('U')\n","# Create feature vectors\n","vectorizer_uni = TfidfVectorizer(ngram_range = (1,1))\n","X = vectorizer_uni.fit_transform(X)\n","y = selected_df['Sentiment']\n","\n","\n","\n","X_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.3 , random_state = 0)\n","\n","\n","\n","#test_vectors = vectorizer.transform(selected_df['ts_reviews_stars'])"],"metadata":{"id":"8HHjfnPHVDqK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train.shape , X_test.shape"],"metadata":{"id":"XAvYOgtUVDuO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#creating a linear svm model"],"metadata":{"id":"nEMJViBvVDyB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#neccesary\n","\n","\n","import time\n","from sklearn.svm import LinearSVC\n","from sklearn.metrics import classification_report\n","# Perform classification with SVM, kernel=linear\n","classifier_linear = LinearSVC()\n","t0 = time.time()\n","classifier_linear.fit(X_train,y_train)\n","t1 = time.time()\n","prediction_linear = classifier_linear.predict(X_test)\n","t2 = time.time()\n","time_linear_train = t1-t0\n","time_linear_predict = t2-t1\n","# results\n","print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n","report = classification_report(y_test, prediction_linear)\n","print('Accuracy of report (1,1) unigram :' ,report)\n","# print('positive: ', report['Positive'])\n","# print('negative: ', report['Negative'])"],"metadata":{"id":"nmypdEpZVD2Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","confusion_matrix_svm = confusion_matrix(y_test, prediction_linear, labels=['Positive','Negative'])\n","confusion_matrix_svm"],"metadata":{"id":"_u1sOBJQVD6F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","# plot the confusion matrix\n","ax = plt.axes()\n","sns.heatmap(confusion_matrix_svm, annot=True, fmt=\"d\")\n","ax.set_title('Confusion matrix svm of Unigrams')"],"metadata":{"id":"IzoM9p3WVD9p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","X = selected_df['Reviews_text'].values.astype('U')\n","# Create feature vectors\n","vectorizer_bi = TfidfVectorizer(ngram_range = (1,2))\n","X = vectorizer_bi.fit_transform(X)\n","y = selected_df['Sentiment']\n","\n","\n","\n","X_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.3 , random_state = 0)"],"metadata":{"id":"ZgZvSwcHVEHC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","from sklearn.svm import LinearSVC\n","from sklearn.metrics import classification_report\n","# Perform classification with SVM, kernel=linear\n","classifier_linear = LinearSVC()\n","t0 = time.time()\n","classifier_linear.fit(X_train,y_train)\n","t1 = time.time()\n","prediction_linear = classifier_linear.predict(X_test)\n","t2 = time.time()\n","time_linear_train = t1-t0\n","time_linear_predict = t2-t1\n","# results\n","print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n","report = classification_report(y_test, prediction_linear)\n","print('Accuracy of report (2,2) bigram :' ,report)\n","# print('positive: ', report['Positive'])\n","# print('negative: ', report['Negative'])"],"metadata":{"id":"6TaHT_O8VEZO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","confusion_matrix_svm = confusion_matrix(y_test, prediction_linear, labels=['Positive','Negative'])\n","confusion_matrix_svm"],"metadata":{"id":"iv39pc5eVEce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","# plot the confusion matrix\n","ax = plt.axes()\n","sns.heatmap(confusion_matrix_svm, annot=True, fmt=\"d\")\n","ax.set_title('Confusion matrix svm for Bigrams')"],"metadata":{"id":"y7LucmBsVEf5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# results = pd.DataFrame({\n","#     'Actual': np.array(y_test).flatten(),\n","#     'Predicted': np.array(y_pred).flatten(),\n","# })\n","# results[1:20]"],"metadata":{"id":"Ji_Z-6G3VEjJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Assigning the parameters and its values which need to be tuned.\n","# from sklearn.model_selection import GridSearchCV\n","# from sklearn.pipeline import Pipeline\n","# X = selected_df['Reviews_text']\n","# y = selected_df['Sentiment']\n","# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n","# param_grid = {\"kernel\" : ['linear', 'poly', 'rbf', 'sigmoid'],\n","#              \"max_iter\" : [1,10,20],\n","#              'C' : np.arange(0,20,1)}\n","\n","# # Fitting the SVM model\n","# modelsvc = LinearSVC()\n","\n","# # Performing the GridSearchCV\n","# clf = GridSearchCV(modelsvc, param_grid)\n","# clf.fit(X_train, y_train)"],"metadata":{"id":"Fn02icjzVEmE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #Find accuracy for trigram\n","# from sklearn.feature_extraction.text import TfidfVectorizer\n","# from sklearn.model_selection import train_test_split\n","# X = selected_df['Reviews_text'].values.astype('U')\n","# # Create feature vectors\n","# vectorizer_tri = TfidfVectorizer(\n","#                              ngram_range = (3,3))\n","# X = vectorizer_tri.fit_transform(X)\n","\n","# y = selected_df['Sentiment']\n","\n","\n","\n","# X_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.3 , random_state = 0)"],"metadata":{"id":"1u8EXM7EVEpP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import time\n","# from sklearn.svm import LinearSVC\n","# from sklearn.metrics import classification_report\n","# # Perform classification with SVM, kernel=linear\n","# classifier_linear = LinearSVC()\n","# t0 = time.time()\n","# classifier_linear.fit(X_train,y_train)\n","\n","\n","# t1 = time.time()\n","# prediction_linear = classifier_linear.predict(X_test)\n","# t2 = time.time()\n","# time_linear_train = t1-t0\n","# time_linear_predict = t2-t1\n","# # results\n","# print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n","# result = classification_report(y_test, prediction_linear)\n","# print('Accuracy of result (3,3) bigram :' ,report)\n","# # https://medium.com/@vasista/sentiment-analysis-using-svm-338d418e3ff1    help\n","# # print('positive: ', report['Positive'])\n","# # print('negative: ', report['Negative'])"],"metadata":{"id":"7a7OCXVwVEtA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#plt.scatter(X_train[:,0],X_train[:,1], c = y,  s = 30, cmap = plt.cm.Paired)"],"metadata":{"id":"N9qUfFG3VExM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # https://www.youtube.com/watch?v=AekvfY5Rnlc&ab_channel=KGPTalkie              help\n","# # x = get_clean(x)\n","# # vec = tfidf.transform([x])\n","# # clf.predic(vec)\n","# from sklearn.metrics import confusion_matrix\n","# confusion_matrix_svm = confusion_matrix(y_test, prediction_linear, labels=['Positive','Negative'])\n","# confusion_matrix_svm"],"metadata":{"id":"fUIFqj6YVE06"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import seaborn as sns\n","# # plot the confusion matrix\n","# ax = plt.axes()\n","# sns.heatmap(confusion_matrix_svm, annot=True, fmt=\"d\")\n","# ax.set_title('Confusion matrix svm')"],"metadata":{"id":"9C0JJUWjVE33"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''*******End of SVM Model*******'''"],"metadata":{"id":"4EomrkuGVE70"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# review_vector = vectorizer.transform(test_df['ts_reviews']) # vectorizing\n","# print(classifier_linear.predict(review_vector))"],"metadata":{"id":"wz91d5PWVE_N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Logestic Rregression"],"metadata":{"id":"vmltyT2rVFCc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["selected_df.head(10)"],"metadata":{"id":"MPC1EFyRVFFu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# selected_df['Sentiment'] =  selected_df['Reviews_text_stars'].replace({'Positive': 1, 'Negative': 0})\n","# selected_df"],"metadata":{"id":"dV4hW6n4VFJJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["selected_df['Reviews_text']"],"metadata":{"id":"dg1gfKV4VFNo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download(\"all\")\n","!pip install nltk\n","import re\n","from nltk.corpus import stopwords\n","from  nltk.stem import SnowballStemmer\n","stop_words = stopwords.words(\"english\")\n","stemmer = SnowballStemmer(\"english\")\n","TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\""],"metadata":{"id":"yOvjp3jyVFRL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def preprocess(Reviews_text, stem=False):\n","    # Remove link,user and special characters\n","    text = re.sub(TEXT_CLEANING_RE, ' ', str(Reviews_text).lower()).strip()\n","    tokens = []\n","    for token in text.split():\n","        if token not in stop_words:\n","            if stem:\n","                tokens.append(stemmer.stem(token))\n","            else:\n","                tokens.append(token)\n","    return \" \".join(tokens)"],"metadata":{"id":"ppnDUqGCVFUD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","selected_df.Reviews_text = selected_df.Reviews_text.apply(lambda x: preprocess(x))"],"metadata":{"id":"D4zjE1VFWfWg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","X = selected_df['Reviews_text'].values.astype('U')\n","# Create feature vectors\n","vectorizer_lr_uni = TfidfVectorizer(min_df = 5,\n","                             max_df = 0.8,\n","                             max_features = 10000,\n","                             ngram_range = (1,1))\n","X = vectorizer_lr_uni.fit_transform(X)\n","y = selected_df['Sentiment']\n","\n","\n","\n","X_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.3 , random_state = 0)"],"metadata":{"id":"C2InPIXkWfaQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#   https://github.com/llSourcell/logistic_regression/blob/master/Sentiment%20analysis%20with%20Logistic%20Regression.ipynb\n","#Bag of words of Dataset\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","count = CountVectorizer()\n","bag = count.fit_transform(selected_df['Reviews_text'])\n","\n","count.vocabulary_"],"metadata":{"id":"eGdonwSeWfeR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bag.toarray()"],"metadata":{"id":"G5S8v9jWWfjm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","\n","from sklearn.feature_extraction.text import TfidfTransformer\n","\n","\n","tfidf = TfidfTransformer(use_idf=True,\n","                         norm='l2',\n","                         smooth_idf=True)\n","\n","np.set_printoptions(precision=2)\n","\n","# Feed the tf-idf transformer with our previously created Bag of Words\n","tfidf.fit_transform(bag).toarray()"],"metadata":{"id":"NIeoMvnzWfoi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["selected_df['Reviews_text'].astype(str)"],"metadata":{"id":"nPMyPZI1WfsG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Removing stop words\n","#Now that we know how to format and score our input, we can start doing the analysis! Can we?... Well, we can, but let's look at our real vocabulary. Specifically, the most common words:\n","from nltk.corpus import stopwords\n","filtered_words = [word for word in selected_df['Reviews_text'] if word not in stopwords.words('english')]\n","filtered_words"],"metadata":{"id":"1yI0TzC_WfvK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","#selected_df.dropna(axis=1)\n","no_vacb = Counter()\n","selected_df['Reviews_text'].str.lower().str.split().apply(no_vacb.update).value_counts()\n","print(no_vacb)\n","from nltk.corpus import stopwords\n","stop = stopwords.words('english')\n","vocab = {}\n","vocab_reduced = Counter()\n","for w, c in vocab.items():\n","    if not w in stop:\n","        vocab_reduced[w]=c\n","\n","vocab_reduced.most_common(10)"],"metadata":{"id":"-458s8JFWfy0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","def preprocessor(filtered_words):\n","    \"\"\" Return a cleaned version of text\n","    \"\"\"\n","    # Remove HTML markup\n","    filtered_words = re.sub('<[^>]*>', '', filtered_words)\n","    # Save emoticons for later appending\n","    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', filtered_words)\n","    # Remove any non-word character and append the emoticons,\n","    # removing the nose character for standarization. Convert to lower case\n","    filtered_words = (re.sub('[\\W]+', ' ', filtered_words.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n","    return filtered_words"],"metadata":{"id":"hLEBrPvAWf12"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#We also need a tokenizer to break down our twits in individual words. We will implement two tokenizers, a regular one and one that does steaming\n","from nltk.stem import PorterStemmer\n","\n","porter = PorterStemmer()\n","\n","def tokenizer(filtered_words):\n","    return filtered_words.split()\n","\n","def tokenizer_porter(filtered_words):\n","    return [porter.stem(word) for word in filtered_words.split()]\n","\n","print(tokenizer('Hi there, I am ibad, and I like coding'))\n","print(tokenizer_porter('Hi there, I am loving this, like with a lot of love'))"],"metadata":{"id":"RmljLQAMWf5M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","selected_df.isnull().sum()\n","# split the dataset in train and test\n","X = selected_df['Reviews_text']\n","y = selected_df['Sentiment']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)"],"metadata":{"id":"z2r6WqEpWf8w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #neccesary\n","# # Same spliting for logestic regression as SVM\n","# X = selected_df['Reviews_text']\n","# y = selected_df['Sentiment']\n","# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n","\n","# X = selected_df['Reviews_text'].values.astype('U')\n","# from sklearn.model_selection import GridSearchCV\n","# from sklearn.pipeline import Pipeline\n","# from sklearn.linear_model import LogisticRegression\n","# from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# tfidf = TfidfVectorizer(strip_accents=None,\n","#                         lowercase=False,\n","#                         preprocessor=None)\n","\n","# param_grid = [{'vect__ngram_range': [(1, 1)],\n","#                'vect__stop_words': [stop, None],\n","#                'vect__tokenizer': [tokenizer, tokenizer_porter],\n","#                'vect__preprocessor': [None, preprocessor],\n","#                'clf__penalty': ['l1', 'l2'],\n","#                'clf__C': [1.0, 10.0, 100.0]},\n","#               {'vect__ngram_range': [(1, 1)],\n","#                'vect__stop_words': [stop, None],\n","#                'vect__tokenizer': [tokenizer, tokenizer_porter],\n","#                'vect__preprocessor': [None, preprocessor],\n","#                'vect__use_idf':[False],\n","#                'vect__norm':[None],\n","#                'clf__penalty': ['l1', 'l2'],\n","#                'clf__C': [1.0, 10.0, 100.0]},\n","#               ]\n","\n","# lr_tfidf = Pipeline([('vect', tfidf),\n","#                      ('clf', LogisticRegression(random_state=0))])\n","\n","# gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n","#                            scoring='accuracy',\n","#                            cv=5,\n","#                            verbose=1,\n","#                            n_jobs=-1)\n","# # Note: This may take a long while\n","\n","\n","# #test_vectors = vectorizer.transform(selected_df['ts_reviews_stars'])"],"metadata":{"id":"rLIWdAEBWgAV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# gs_lr_tfidf.fit(X_train, y_train)\n"],"metadata":{"id":"izTlQedTWgDo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # gs_lr_tfidf.fit(X_train, y_train)\n","# print('Best parameter set: ' + str(gs_lr_tfidf.best_params_))\n","# print('Best accuracy: %.3f for unigram' % gs_lr_tfidf.best_score_)"],"metadata":{"id":"dPBsLayGWgHA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# clf = gs_lr_tfidf.best_estimator_\n","# print('Accuracy in test: %.3f' % clf.score(X_test, y_test))"],"metadata":{"id":"2U8uBZJmWgKF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # practice for checking\n","# twits = [\n","#     \"This is really bad, I don't like it at all\",\n","#     \"I love this!\",\n","#     \":)\",\n","#     \"This is so bad... :(\"\n","# ]\n","\n","# preds = clf.predict(twits)\n","\n","# for i in range(len(twits)):\n","#     print(f'{twits[i]} --> {preds[i]}')\n","\n","\n","# # #https://github.com/llSourcell/logistic_regression/blob/master/Sentiment%20analysis%20with%20Logistic%20Regression.ipynb help"],"metadata":{"id":"khTxsMzWWgNw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### RF\n","#https://www.kaggle.com/code/viroviro/sentiment-analysis-tf-idf-logistic-regression"],"metadata":{"id":"AqmMn6DrWgRK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #Find accuracy for bigram\n","# # Same spliting for logestic regression as SVM\n","# X = selected_df['Reviews_text']\n","# y = selected_df['Sentiment']\n","# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n","\n","# X = selected_df['Reviews_text'].values.astype('U')\n","# from sklearn.model_selection import GridSearchCV\n","# from sklearn.pipeline import Pipeline\n","# from sklearn.linear_model import LogisticRegression\n","# from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# tfidf = TfidfVectorizer(strip_accents=None,\n","#                         lowercase=False,\n","#                         preprocessor=None)\n","\n","# param_grid = [{'vect__ngram_range': [(2,2)],\n","#                'vect__stop_words': [stop, None],\n","#                'vect__tokenizer': [tokenizer, tokenizer_porter],\n","#                'vect__preprocessor': [None, preprocessor],\n","#                'clf__penalty': ['l1', 'l2'],\n","#                'clf__C': [1.0, 10.0, 100.0]},\n","#               {'vect__ngram_range': [(2,2)],\n","#                'vect__stop_words': [stop, None],\n","#                'vect__tokenizer': [tokenizer, tokenizer_porter],\n","#                'vect__preprocessor': [None, preprocessor],\n","#                'vect__use_idf':[False],\n","#                'vect__norm':[None],\n","#                'clf__penalty': ['l1', 'l2'],\n","#                'clf__C': [1.0, 10.0, 100.0]},\n","#               ]\n","\n","# lr_tfidf = Pipeline([('vect', tfidf),\n","#                      ('clf', LogisticRegression(random_state=0))])\n","\n","# gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n","#                            scoring='accuracy',\n","#                            cv=5,\n","#                            verbose=1,\n","#                            n_jobs=-1)\n","# # Note: This may take a long while\n","\n","\n","# #test_vectors = vectorizer.transform(selected_df['ts_reviews_stars'])"],"metadata":{"id":"5bwJiNZVWgUZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# gs_lr_tfidf.fit(X_train, y_train)"],"metadata":{"id":"buCLCo5PWgYr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #52000\n","# # gs_lr_tfidf.fit(X_train, y_train)\n","# print('Best parameter set: ' + str(gs_lr_tfidf.best_params_))\n","# print('Best accuracy: %.3f for bigram' % gs_lr_tfidf.best_score_)"],"metadata":{"id":"Gstd6eF_WgfB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Random Forest"],"metadata":{"id":"8NQGp_P1Wgj0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pd.set_option('display.max_rows', None)\n","# selected_df\n","#X_train = X_train.drop(labels=[7400], axis=0)"],"metadata":{"id":"ib4dxeCPWgnN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import re\n","\n","# def preprocessor(filtered_words):\n","#     \"\"\" Return a cleaned version of text\n","#     \"\"\"\n","#     # Remove HTML markup\n","#     filtered_words = re.sub('<[^>]*>', '', str(filtered_words))\n","#     # Save emoticons for later appending\n","#     emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', filtered_words)\n","#     # Remove any non-word character and append the emoticons,\n","#     # removing the nose character for standarization. Convert to lower case\n","#     filtered_words = (re.sub('[\\W]+', ' ', filtered_words.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n","#     return filtered_words\n","\n","# preprocessor(X_train)\n","#selected_df = selected_df[52000:].copy()\n","selected_df"],"metadata":{"id":"MXomVGmnWgqN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","X = selected_df['Reviews_text'].values.astype('U')\n","# Create feature vectors\n","vectorizer_bi = TfidfVectorizer(ngram_range = (2,2))\n","X = vectorizer_bi.fit_transform(X)\n","y = selected_df['Sentiment']\n","\n","\n","\n","X_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.3 , random_state = 0)\n"],"metadata":{"id":"LO45H3gNWgtB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train model random forest\n","from sklearn.ensemble import RandomForestClassifier\n","# Initialize a Random Forest classifier with 500 trees\n","forest = RandomForestClassifier(n_estimators = 5000, max_depth = None, min_samples_split=2, min_samples_leaf =1,\n","                                bootstrap = True, random_state=0)\n","# Train the model\n","\n","forest = forest.fit(X_train, y_train)\n","# Print score of model(using test dataset)\n","print(\"Accuracy of Random Forest unigram :\", forest.score(X_test, y_test))"],"metadata":{"id":"v7ixKRO9WgwK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred_forest  = forest.predict(X_test)\n","# y_pred_logistic  = logistic.predict(X_test)\n","y_pred_forest"],"metadata":{"id":"KDnk9fCRWgzT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","confusion_matrix_forest = confusion_matrix(y_test, y_pred_forest, labels=['Positive','Negative'])\n","confusion_matrix_forest"],"metadata":{"id":"fSwpvSjrWg2P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","# plot the confusion matrix\n","ax = plt.axes()\n","sns.heatmap(confusion_matrix_forest, annot=True, fmt=\"d\")\n","ax.set_title('Confusion matrix Random Forest for unigram')"],"metadata":{"id":"4mZFumpIWg4_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#find accuracy for bigram\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","X = selected_df['Reviews_text'].values.astype('U')\n","# Create feature vectors\n","vectorizer = TfidfVectorizer(ngram_range = (2,2))\n","X = vectorizer.fit_transform(X)\n","y = selected_df['Sentiment']\n","\n","\n","\n","X_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.3 , random_state = 0)\n"],"metadata":{"id":"wcu_V2O0Wg7_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from sklearn.ensemble import RandomForestClassifier\n","# Initialize a Random Forest classifier with 500 trees\n","forest = RandomForestClassifier(n_estimators = 400, max_depth = None, min_samples_split=2, min_samples_leaf =1,\n","                                bootstrap = True, random_state=0)\n","# Train the model\n","\n","forest = forest.fit(X_train, y_train)\n","# Print score of model(using test dataset)\n","print(\"Accuracy of Random Forest bigram :\", forest.score(X_test, y_test))"],"metadata":{"id":"XIr5y-l3Wg_d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred_forest  = forest.predict(X_test)\n","# y_pred_logistic  = logistic.predict(X_test)\n","y_pred_forest"],"metadata":{"id":"xaWxEM6bYbAH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","confusion_matrix_forest = confusion_matrix(y_test, y_pred_forest, labels=['Positive','Negative'])\n","confusion_matrix_forest"],"metadata":{"id":"36j-ziyBYbSE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Word to vector"],"metadata":{"id":"tgtEN93YWhCT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","import re\n","import joblib\n","from keras.preprocessing.text import Tokenizer\n","import gensim\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.preprocessing import LabelEncoder\n","from keras.layers import Embedding\n","from keras.models import Sequential\n","from keras.layers import Dense,LSTM,Dropout\n","from sklearn.metrics import confusion_matrix,accuracy_score,classification_report"],"metadata":{"id":"zTSR3mvdYkOQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","import re\n","import joblib\n","from keras.preprocessing.text import Tokenizer\n","import gensim\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.preprocessing import LabelEncoder\n","from keras.layers import Embedding\n","from keras.models import Sequential\n","from keras.layers import Dense,LSTM,Dropout\n","from sklearn.metrics import confusion_matrix,accuracy_score,classification_report"],"metadata":{"id":"Ams8QNxrYkuV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mIaNeiqsYk0y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7MLkVlb8Yk6c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bpMqPkLDYk9x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"d5aYNWVmYlCE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qPWc6fjvYlGI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vDfFd5nVYlJF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7TRgAdUcYlMh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vutwj02PYlPZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NKwwezQuYlTL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ecmxPk65YlWW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vQXaXN8ZYld0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_5DJnh6tYlhd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GxdY1-GTYllv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7EIXlFnRYlpx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Trmg7ZmCYlte"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cmTGCQJtYlxP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ysep_fK5Yl1X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xuIFVWJNYl50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Na_IlQJKYl-Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vDBuqv7QYmF1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3_3aE8s1YmMf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9Cjsf3zwYmQ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MnaU0ilGYmVj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_8ybmIKIYmZi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"t3OLpwJqYmd0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sQT2oVRWYmiG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WPaHDDLTYmmC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5IrWWCsKYmqF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Vh-RebuJYmuT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NLa6CH0_YmyR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-oj1m2YcYm2T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qpblJLYRYm6S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0fziMqwYYm-h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CD901kv5YnEA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XmyHuUGfYnH4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"A6be5xKUWhFf"},"execution_count":null,"outputs":[]}]}